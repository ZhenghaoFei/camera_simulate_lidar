{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_filename_queue(file_numbers, data_dir):\n",
    "    # reconstruce filename for three source\n",
    "    left_image_filename_list = []\n",
    "    right_image_filename_list = []\n",
    "    lidar_filename_list = []\n",
    "\n",
    "    for i in range(len(file_numbers)):\n",
    "        left_image_filename = data_dir +  file_numbers[i] + \"left.png\"\n",
    "        right_image_filename = data_dir +  file_numbers[i] + \"right.png\"\n",
    "        lidar_filename = data_dir + file_numbers[i] + \".txt\"\n",
    "        left_image_filename_list.append(left_image_filename)\n",
    "        right_image_filename_list.append(right_image_filename)\n",
    "        lidar_filename_list.append(lidar_filename)\n",
    "\n",
    "    # list to tensor queue\n",
    "    left_image_filename_queue = tf.train.string_input_producer(left_image_filename_list)\n",
    "    right_image_filename_queue = tf.train.string_input_producer(right_image_filename_list)\n",
    "    lidar_filename_queue = tf.train.string_input_producer(lidar_filename_list)\n",
    "\n",
    "    return [left_image_filename_queue, right_image_filename_queue, lidar_filename_queue]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(left_image_filename_queue, right_image_filename_queue, lidar_filename_queue):\n",
    "\n",
    "    # creat three class \n",
    "    class Left_Image_Record(object):\n",
    "        pass\n",
    "    Left_Image = Left_Image_Record()\n",
    "\n",
    "    class Right_Image_Record(object):\n",
    "        pass\n",
    "    Right_Image = Right_Image_Record()\n",
    "\n",
    "    class Lidar_Record(object):\n",
    "        pass\n",
    "    Lidar = Lidar_Record()\n",
    "\n",
    "    # read and decode image    \n",
    "    image_reader = tf.WholeFileReader()\n",
    "    Left_Image.key, left_value = image_reader.read(left_image_filename_queue)\n",
    "    # Left_Image.image = tf.image.decode_png(left_value)\n",
    "    Left_Image.uint8image  = tf.reshape(tf.image.decode_png(left_value), [ORIGINAL_IMAGE_HEIGHT, ORIGINAL_IMAGE_WIDTH, IMAGE_DEPTH])\n",
    "\n",
    "    Right_Image.key, right_value = image_reader.read(right_image_filename_queue)\n",
    "    # Right_Image.image = tf.image.decode_png(right_value)\n",
    "    Right_Image.uint8image  = tf.reshape(tf.image.decode_png(right_value), [ORIGINAL_IMAGE_HEIGHT, ORIGINAL_IMAGE_WIDTH, IMAGE_DEPTH])\n",
    "\n",
    "    # read and decode lidar    \n",
    "    lidar_reader = tf.TextLineReader()\n",
    "    Lidar.key, lidar_value = lidar_reader.read(lidar_filename_queue)\n",
    "    record_defaults = [[] for col in range(120)]\n",
    "    lidar_samples = tf.decode_csv(lidar_value, record_defaults=record_defaults)\n",
    "    Lidar.info = tf.pack(lidar_samples)\n",
    "\n",
    "    return [Left_Image, Right_Image, Lidar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _generate_image_and_label_batch(left_image, right_image, lidar, min_queue_examples, batch_size, shuffle):\n",
    "    \"\"\"Construct a queued batch of images and labels.\n",
    "\n",
    "    Args:\n",
    "    left_image: 3-D Tensor of [height, width, 3] of type.float32.\n",
    "    right_image: 3-D Tensor of [height, width, 3] of type.float32.\n",
    "    lidar: 1-D Tensor of [lidar_size] type.float32\n",
    "    min_queue_examples: int32, minimum number of samples to retain\n",
    "      in the queue that provides of batches of examples.\n",
    "    batch_size: Number of images per batch.\n",
    "    shuffle: boolean indicating whether to use a shuffling queue.\n",
    "\n",
    "    Returns:\n",
    "    left_image_batch: Images. 4D tensor of [batch_size, height, width, 3] size.\n",
    "    right_image_batch: Images. 4D tensor of [batch_size, height, width, 3] size.\n",
    "    lidar: Labels. 2D tensor of [batch_size, lidar_size] size.\n",
    "    \"\"\"\n",
    "    # Create a queue that shuffles the examples\n",
    "    # min_queue_examples = 100\n",
    "    num_preprocess_threads = 8\n",
    "    if shuffle:\n",
    "        left_batch, right_batch, lidar_batch = tf.train.shuffle_batch(\n",
    "            [left_image, right_image, lidar],\n",
    "            batch_size=batch_size,\n",
    "            num_threads=num_preprocess_threads,\n",
    "            capacity=min_queue_examples + 3 * batch_size,\n",
    "            min_after_dequeue=min_queue_examples)\n",
    "    else:\n",
    "        left_batch, right_batch, lidar_batch = tf.train.batch(\n",
    "            [left_image, right_image, lidar],\n",
    "            batch_size=batch_size,\n",
    "            num_threads=num_preprocess_threads,\n",
    "            capacity=min_queue_examples + 3 * batch_size)\n",
    "\n",
    "    # Display the training images in the visualizer.\n",
    "    tf.image_summary('left_images', left_batch)\n",
    "    tf.image_summary('right_images', right_batch)\n",
    "\n",
    "    return left_batch, right_batch, lidar_batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only get serial number\n",
    "def get_filenumber(mypath):\n",
    "    # mypath = './data/data1/'\n",
    "    data_lidar = sorted(glob.glob(mypath + '*txt'), key=os.path.basename)\n",
    "    print (mypath)\n",
    "\n",
    "    file_numbers = []\n",
    "    for i in range(len(data_lidar)):\n",
    "        number = data_lidar[i]\n",
    "        number = number.replace(mypath,\"\")\n",
    "        number = number.replace(\".txt\",\"\")\n",
    "        file_numbers.append(number)\n",
    "    print (\"number of sample: \", len(file_numbers))\n",
    "    return file_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = './data/data1/'\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 1\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 1\n",
    "\n",
    "IMAGE_HEIGHT = 16\n",
    "IMAGE_WIDTH = 64\n",
    "\n",
    "ORIGINAL_IMAGE_HEIGHT = 128\n",
    "ORIGINAL_IMAGE_WIDTH = 672\n",
    "IMAGE_DEPTH = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/data1/\n",
      "('number of sample: ', 1219)\n",
      "Tensor(\"ReaderRead:0\", shape=(), dtype=string)\n",
      "Tensor(\"ReaderRead_1:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Start a new session to show example output.\n",
    "with tf.Session() as sess:\n",
    "    # Required to get the filename matching to run.\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    # Coordinate the loading of image files.\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "#     # Get an image tensor and print its value.\n",
    "    file_numbers = get_filenumber(data_dir)\n",
    "    num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\n",
    "    left_image_filename_queue, right_image_filename_queue, lidar_filename_queue = get_filename_queue(file_numbers, data_dir)\n",
    "    Left_Image, Right_Image, Lidar = read_data(left_image_filename_queue, right_image_filename_queue, lidar_filename_queue)\n",
    "#     left_image = tf.cast(Left_Image.uint8image, tf.float32)\n",
    "#     right_image = tf.cast(Right_Image.uint8image, tf.float32)\n",
    "    print (Left_Image.key)\n",
    "    print (Right_Image.key)\n",
    "    leftkey = sess.run(Right_Image.key)\n",
    "    print (leftkey)\n",
    "#     tf.Print(Left_Image.key, [Left_Image.key])\n",
    "#     tf.Print(Right_Image.key, [Right_Image.key])\n",
    "#     print (Lidar.key.eval())\n",
    "#     image_tensor = sess.run(left_image) # read single image\n",
    "#     print image_tensor.shape\n",
    "#     plt.imshow(image_tensor)\n",
    "\n",
    "    # Get lidar info\n",
    "#     lidar_value = sess.run(lidar_label)\n",
    "#     lidar_np = np.fromstring(lidar_string, sep='\\n')\n",
    "#     print(lidar_label.eval())\n",
    "    # Finish off the filename queue coordinator.\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
